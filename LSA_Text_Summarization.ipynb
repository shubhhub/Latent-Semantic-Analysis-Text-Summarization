{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cddb505-7166-4771-88e8-e581bcfd000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author : Shubh Garg\n",
    "# Github : https://github.com/shubhhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85d25a23-74ea-487e-b57c-197f43af61a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Semantic Analysis uses the following steps:\n",
    "# Step 1: Preprocessing of text data, tokenizing and all\n",
    "# Step 2: Formation of term document matrix, tf-idf vectorization \n",
    "# Step 3: Applying Singular Value Decomposition to factorize tfidf vector\n",
    "# Step 4: Creatinng Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b38c2710-0442-4f81-a928-82518476e111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Installation Complete\n"
     ]
    }
   ],
   "source": [
    "# Install all the necessary python libraries\n",
    "!pip install numpy scikit-learn\n",
    "\n",
    "print(\"Installation Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d9ad2df-696a-4e9a-9c48-b462fd3739af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the necessary libraries have been imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import all the necessary libraries\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import Counter\n",
    "\n",
    "print(\"All the necessary libraries have been imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18e8dd4f-884c-4ed4-8a52-fe4479301131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing functions ready to use\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the text file obtained \n",
    "# It returns the lower cased tokenized array \n",
    "# Example:\n",
    "# If my sentence is \"Hi, my name is Shubh Garg. I am currently doing my BTech from JIIT.\"\n",
    "# Then it will return [[\"hi my name is shubh garg\"], [\" i am currently doing my btech from jiit\"]]\n",
    "# Space before 'i' in 2nd sentence as splitting on fullstop\n",
    "\n",
    "def is_alphanumeric(ch):\n",
    "    # Function checks if character is either alphabet or number \n",
    "    return (ch>='a' and ch<='z') or (ch>='A' and ch<='Z') or (ch>='0' and ch<='9')\n",
    "\n",
    "def is_space_or_underscore(ch):\n",
    "    # Function checks if character is a black space, underscore or not\n",
    "    return ch == ' ' or ch == '_'\n",
    "\n",
    "def is_fullstop(ch):\n",
    "    # Function checks if character is fullstop or not\n",
    "    return ch == '.'\n",
    "\n",
    "def is_valid_char(ch):\n",
    "    # Checks validity of character\n",
    "    # Note: Change here if you want to get other types of characters in you paragraph\n",
    "    # For example, if you want hyphen(-), then write a 'is_hyphen()' function and add ' or is_hyphen(ch)' in the return statement\n",
    "    return is_alphanumeric(ch) or is_space_or_underscore(ch) or is_fullstop(ch)\n",
    "\n",
    "def is_upper_to_lower(ch):\n",
    "    # Checks and convert upper case characters to lower case characters\n",
    "    if (ch>='A' and ch<='Z'):\n",
    "        ch = chr(ord(ch) + ord('a') - ord('A')) #ord is used to get ASCII value, and chr is used to convert from ASCII to char\n",
    "    return ch\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert everything to lower case and remove unnecessary characters\n",
    "    lowercase_text = \"\"\n",
    "    for ch in text:\n",
    "        if is_valid_char(ch):\n",
    "            lowercase_text += is_upper_to_lower(ch)\n",
    "\n",
    "    # Split the array of sentences, based of fullstop\n",
    "    split_sentences = lowercase_text.split('.')\n",
    "\n",
    "    return split_sentences\n",
    "\n",
    "def tokenize_document(sentences):\n",
    "    tokenized_documents = [sentence.lower().split() for sentence in sentences]\n",
    "    \n",
    "    # We are dropping single character words like 'a' or 'I' in the next code line as they don't give much value\n",
    "    # So it return: [[\"hi\", \"my\", \"name\", \"is\", \"shubh garg\"], [\"am\", \"currently\", \"doing\", \"my\", \"btech\", \"from\", \"jiit\"]]\n",
    "    # It dropped 'i'\n",
    "    tokenized_documents = [[word for word in sub_list if len(word) > 1] for sub_list in tokenized_documents]\n",
    "\n",
    "    # Note: You can also use stopwords here as they also don't add muchh value\n",
    "    \n",
    "    return tokenized_documents\n",
    "\n",
    "print(\"Preprocessing functions ready to use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e77beb6c-79cd-46b7-b94c-cd88976a5728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDM functions are ready to use\n"
     ]
    }
   ],
   "source": [
    "def calculate_tf(word, document):\n",
    "    # TF is calculated as (number of times word is present in the document)/(total number of words in the document)\n",
    "    word_count = document.count(word)\n",
    "    total_words = len(document)\n",
    "    tf = word_count / total_words if total_words > 0 else 0\n",
    "    return tf\n",
    "\n",
    "def calculate_idf(word, documents):\n",
    "    # IDF is (total number of documents or sentences)/(number of documents in which word is present)\n",
    "    num_documents_with_term = sum(1 for document in documents if word in document)\n",
    "    total_documents = len(documents)\n",
    "    # Adding 1 in division for edge case num_documents_with_term = 0\n",
    "    # Adding 1 overall to deal with negative values of log\n",
    "    # I refered GFG here, you can also search TFIDF calculation on GFG\n",
    "    idf = 1 + math.log((total_documents + 1) / (1 + num_documents_with_term))\n",
    "    return idf\n",
    "\n",
    "def create_term_document_matrix(tokenized_documents):\n",
    "    # Get unique words\n",
    "    unique_words = sorted(list(set(word for document in tokenized_documents for word in document)))\n",
    "    \n",
    "    # Calculate TF-IDF values\n",
    "    term_document_matrix = []\n",
    "    for document in tokenized_documents:\n",
    "        tf_idf_values = [calculate_tf(word, document) * calculate_idf(word, tokenized_documents) for word in unique_words]\n",
    "        term_document_matrix.append(tf_idf_values)\n",
    "    return term_document_matrix\n",
    "\n",
    "print(\"TDM functions are ready to use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d16d73b2-aa58-4b94-b4e4-ce3f79133351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA function is ready to use\n"
     ]
    }
   ],
   "source": [
    "# LSA on TDM is pretty standart process\n",
    "# Use SVD from Linear Algebra provided by numpy\n",
    "# It requires eigenvectors and eigenvalues, which, if you want to create the matrix, that again will use Linear Algebra to ease the work\n",
    "def apply_lsa(term_document_matrix, k):\n",
    "    # Perform Singular Value Decomposition (SVD)\n",
    "    U, S, Vt = np.linalg.svd(term_document_matrix, full_matrices=False)\n",
    "\n",
    "    # Keep only the top k singular values and vectors\n",
    "    U_k = U[:, :k]\n",
    "    S_k = np.diag(S[:k])\n",
    "    Vt_k = Vt[:k, :]\n",
    "\n",
    "    # Compute the reduced term-document matrix\n",
    "    reduced_term_document_matrix = np.dot(U_k, np.dot(S_k, Vt_k))\n",
    "\n",
    "    return reduced_term_document_matrix\n",
    "\n",
    "print(\"LSA function is ready to use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c76d232-7081-4d2c-ae5b-a2e418270781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions to find best sentences and generate summart ready to use\n"
     ]
    }
   ],
   "source": [
    "def compute_sentence_scores(reduced_term_document_matrix):\n",
    "\n",
    "    # It is dot multiplicaton, it is more similar to cosine similarity, if you want to search about it\n",
    "    # Note: You have to change it with your own scoring function to find similarities if you don't want dot multiplication\n",
    "    sentence_scores = np.dot(reduced_term_document_matrix, reduced_term_document_matrix.T).diagonal()\n",
    "    return sentence_scores\n",
    "\n",
    "\n",
    "# Select the sentences which have a good score, and then combining them\n",
    "def select_top_sentences(sentence_scores, num_sentences):\n",
    "    # Sort the sentence scores in descending order\n",
    "    sorted_sentence_indices = np.argsort(sentence_scores)[::-1]\n",
    "\n",
    "    # Select the indices of the top num_sentences\n",
    "    top_sentence_indices = sorted_sentence_indices[:num_sentences]\n",
    "    return top_sentence_indices\n",
    "\n",
    "def generate_summary(sentences, top_sentence_indices):\n",
    "    # Initialize the summary\n",
    "    summary = ''\n",
    "\n",
    "    # Concatenate the top sentences\n",
    "    for i in top_sentence_indices:\n",
    "        summary += sentences[i] + '. '\n",
    "\n",
    "    return summary\n",
    "    \n",
    "print(\"Functions to find best sentences and generate summart ready to use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33863eb7-6348-490e-9741-c8b477339169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load the text\n",
    "    # text = \"Hi, my name is Shubh Garg. I am currently doing my BTech from JIIT.\"\n",
    "    # text = \"Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, perception, language understanding, and even decision-making. AI systems are designed to emulate human cognitive functions, utilizing algorithms and data to analyze patterns and make informed decisions. There are two main types of AI: narrow or weak AI, which is designed for a specific task, and general or strong AI, which aims to replicate human cognitive abilities across various domains. Machine learning is a subset of AI that involves training algorithms to recognize patterns and make predictions based on data. Natural Language Processing (NLP) is another essential component of AI, enabling machines to understand, interpret, and generate human-like language. AI applications are widespread and impact various industries, including healthcare, finance, transportation, and entertainment. The ethical implications of AI, such as bias in algorithms and potential job displacement, have also become important considerations. As AI continues to advance, researchers are exploring ways to ensure responsible and transparent development, addressing concerns about privacy, security, and the societal impact of these technologies. The quest for achieving artificial general intelligence, where machines can perform any intellectual task that a human can, remains a long-term goal in the field of AI.\"\n",
    "    text = (input(\"Enter your text: \"))\n",
    "    \n",
    "    # Print the text\n",
    "    \n",
    "    print(\"\\nText:\")\n",
    "    print(text)\n",
    "    \n",
    "    # Preprocess the text\n",
    "    sentences = preprocess_text(text)\n",
    "    # We have not tokenized the sentences here as we need 'sentences' in the later process\n",
    "    # Instead, we have tokinized them in another function\n",
    "    tokenized_document = tokenize_document(sentences)\n",
    "    \n",
    "    # Create the term-document matrix\n",
    "    term_document_matrix = create_term_document_matrix(tokenized_document)\n",
    "\n",
    "    # Apply LSA\n",
    "    k = 10\n",
    "    # k refers to top k singular values and vectors to obtain the reduced term-document matrix\n",
    "    # You can change k\n",
    "    reduced_term_document_matrix = apply_lsa(term_document_matrix, k)\n",
    "\n",
    "    # Select the top sentences for the summary\n",
    "    num_sentences = int(input(\"\\nHow many sentences do you want in your summary: \"))\n",
    "    sentence_scores = compute_sentence_scores(reduced_term_document_matrix)\n",
    "    top_sentence_indices = select_top_sentences(sentence_scores, num_sentences)\n",
    "    summary = generate_summary(sentences, top_sentence_indices)\n",
    "\n",
    "    # Print the summary\n",
    "    print(\"\\nSummary:\")\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abd4c364-9384-4938-8ea3-a214d6619cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your text:  Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, perception, language understanding, and even decision-making. AI systems are designed to emulate human cognitive functions, utilizing algorithms and data to analyze patterns and make informed decisions. There are two main types of AI: narrow or weak AI, which is designed for a specific task, and general or strong AI, which aims to replicate human cognitive abilities across various domains. Machine learning is a subset of AI that involves training algorithms to recognize patterns and make predictions based on data. Natural Language Processing (NLP) is another essential component of AI, enabling machines to understand, interpret, and generate human-like language. AI applications are widespread and impact various industries, including healthcare, finance, transportation, and entertainment. The ethical implications of AI, such as bias in algorithms and potential job displacement, have also become important considerations. As AI continues to advance, researchers are exploring ways to ensure responsible and transparent development, addressing concerns about privacy, security, and the societal impact of these technologies. The quest for achieving artificial general intelligence, where machines can perform any intellectual task that a human can, remains a long-term goal in the field of AI.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text:\n",
      "Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, perception, language understanding, and even decision-making. AI systems are designed to emulate human cognitive functions, utilizing algorithms and data to analyze patterns and make informed decisions. There are two main types of AI: narrow or weak AI, which is designed for a specific task, and general or strong AI, which aims to replicate human cognitive abilities across various domains. Machine learning is a subset of AI that involves training algorithms to recognize patterns and make predictions based on data. Natural Language Processing (NLP) is another essential component of AI, enabling machines to understand, interpret, and generate human-like language. AI applications are widespread and impact various industries, including healthcare, finance, transportation, and entertainment. The ethical implications of AI, such as bias in algorithms and potential job displacement, have also become important considerations. As AI continues to advance, researchers are exploring ways to ensure responsible and transparent development, addressing concerns about privacy, security, and the societal impact of these technologies. The quest for achieving artificial general intelligence, where machines can perform any intellectual task that a human can, remains a long-term goal in the field of AI.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "How many sentences do you want in your summary:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      " these tasks include learning reasoning problemsolving perception language understanding and even decisionmaking.  ai applications are widespread and impact various industries including healthcare finance transportation and entertainment.  natural language processing nlp is another essential component of ai enabling machines to understand interpret and generate humanlike language. \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9515012a-f5b0-4eba-83b9-2fefbbfbf1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
